{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 🐣 Please follow me for new updates https://twitter.com/camenduru\n# 🔥 Please join our discord server https://discord.gg/k5BwmmvJJU\n# 🥳 Please join my patreon community https://patreon.com/camenduru\n\n%cd /kaggle/working\n!wget https://github.com/camenduru/DemoFusion/raw/img/pipeline_demofusion_sdxl.py -O /kaggle/working/pipeline_demofusion_sdxl.py\n\n!pip install -q diffusers accelerate gradio gradio_imageslider pyngrok\n\nimport gradio as gr\nfrom pipeline_demofusion_sdxl import DemoFusionSDXLPipeline\nfrom gradio_imageslider import ImageSlider\nimport torch, gc\nfrom torchvision import transforms\nfrom PIL import Image\n\ndef load_and_process_image(pil_image):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((1024, 1024)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n        ]\n    )\n    image = transform(pil_image)\n    image = image.unsqueeze(0).half()\n    return image\n\n\ndef pad_image(image):\n    w, h = image.size\n    if w == h:\n        return image\n    elif w > h:\n        new_image = Image.new(image.mode, (w, w), (0, 0, 0))\n        pad_w = 0\n        pad_h = (w - h) // 2\n        new_image.paste(image, (0, pad_h))\n        return new_image\n    else:\n        new_image = Image.new(image.mode, (h, h), (0, 0, 0))\n        pad_w = (h - w) // 2\n        pad_h = 0\n        new_image.paste(image, (pad_w, 0))\n        return new_image\n\ndef generate_images(prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, cosine_scale_1, cosine_scale_2, \n                    cosine_scale_3, sigma, view_batch_size, stride, seed, input_image):\n    padded_image = pad_image(input_image).resize((1024, 1024)).convert(\"RGB\")\n    image_lr = load_and_process_image(padded_image).to('cuda')\n    model_ckpt = \"camenduru/DemoFusion\"\n    pipe = DemoFusionSDXLPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16)\n    pipe = pipe.to(\"cuda\")\n    generator = torch.Generator(device='cuda')\n    generator = generator.manual_seed(int(seed))\n    images = pipe(prompt, negative_prompt=negative_prompt, generator=generator,\n                  height=int(height), width=int(width), view_batch_size=int(view_batch_size), stride=int(stride),\n                  num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n                  cosine_scale_1=cosine_scale_1, cosine_scale_2=cosine_scale_2, cosine_scale_3=cosine_scale_3, sigma=sigma,\n                  multi_decoder=True, show_image=False, lowvram=True, image_lr=image_lr\n                 )\n    for i, image in enumerate(images):\n      image.save('image_'+str(i)+'.png')\n    pipe = None\n    gc.collect()\n    torch.cuda.empty_cache()\n    return (images[0], images[-1])\n\nwith gr.Blocks(title=f\"DemoFusion\") as demo:\n    with gr.Column():\n      with gr.Row():\n        with gr.Group():\n          image_input = gr.Image(type=\"pil\", label=\"Input Image\")\n          prompt = gr.Textbox(label=\"Prompt\", value=\"Envision a portrait of an elderly woman, her face a canvas of time, framed by a headscarf with muted tones of rust and cream. Her eyes, blue like faded denim. Her attire, simple yet dignified.\")\n          negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\")\n          width = gr.Slider(minimum=1024, maximum=4096, step=1024, value=2048, label=\"Width\")\n          height = gr.Slider(minimum=1024, maximum=4096, step=1024, value=2048, label=\"Height\")\n          num_inference_steps = gr.Slider(minimum=10, maximum=100, step=1, value=50, label=\"Num Inference Steps\")\n          guidance_scale = gr.Slider(minimum=1, maximum=20, step=0.1, value=7.5, label=\"Guidance Scale\")\n          cosine_scale_1 = gr.Slider(minimum=0, maximum=5, step=0.1, value=3, label=\"Cosine Scale 1\")\n          cosine_scale_2 = gr.Slider(minimum=0, maximum=5, step=0.1, value=1, label=\"Cosine Scale 2\")\n          cosine_scale_3 = gr.Slider(minimum=0, maximum=5, step=0.1, value=1, label=\"Cosine Scale 3\")\n          sigma = gr.Slider(minimum=0.1, maximum=1, step=0.1, value=0.8, label=\"Sigma\")\n          view_batch_size = gr.Slider(minimum=4, maximum=32, step=4, value=4, label=\"View Batch Size\")\n          stride = gr.Slider(minimum=8, maximum=96, step=8, value=64, label=\"Stride\")\n          seed = gr.Number(label=\"Seed\", value=2013)\n          button = gr.Button()\n        output_images = ImageSlider(show_label=False)\n    button.click(fn=generate_images, inputs=[prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, \n                                             cosine_scale_1, cosine_scale_2, cosine_scale_3, sigma, view_batch_size, stride, seed, image_input], outputs=[output_images], show_progress=True)\n\nfrom pyngrok import ngrok, conf\nconf.get_default().auth_token = \"NGROK_TOKEN_HERE\"\npublic_url = ngrok.connect(7860).public_url\nprint(public_url)\n\ndemo.queue().launch(inline=False, share=False, debug=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}